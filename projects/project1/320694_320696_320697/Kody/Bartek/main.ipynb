{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder, FunctionTransformer, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real\n",
    "\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import openml\n",
    "\n",
    "from scipy.stats import uniform, loguniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rn_st = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting preprocessing\n",
    "\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('one-hot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "\n",
    "])\n",
    "\n",
    "col_trans = ColumnTransformer(transformers=[\n",
    "        ('num_pipeline', num_pipeline, make_column_selector(dtype_include=np.number)),\n",
    "        ('cat_pipeline', cat_pipeline, make_column_selector(dtype_include='category'))\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "def label_encode(y):\n",
    "    label_encoder = LabelEncoder()\n",
    "    return label_encoder.fit_transform(y)\n",
    "\n",
    "target_transformer = FunctionTransformer(label_encode, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      " class\n",
      "good    700\n",
      "bad     300\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset 1\n",
    "d1 = openml.datasets.get_dataset(31)\n",
    "X, y, _, _ = d1.get_data(target=d1.default_target_attribute)\n",
    "\n",
    "# print(X.dtypes)\n",
    "# print('Dane:', X.head())\n",
    "# print('Target: ', y.head())\n",
    "\n",
    "class_distribution = y.value_counts()\n",
    "print(\"Class distribution:\\n\", class_distribution)\n",
    "# class_distribution.plot(kind='bar', title=\"Class Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      " class\n",
      "tested_negative    500\n",
      "tested_positive    268\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset 2\n",
    "d2 = openml.datasets.get_dataset(37)\n",
    "X, y, _, _ = d2.get_data(target=d2.default_target_attribute)\n",
    "\n",
    "# print(X.dtypes)\n",
    "# print('Dane:', X.head())\n",
    "# print('Target: ', y.head())\n",
    "\n",
    "class_distribution = y.value_counts()\n",
    "print(\"Class distribution:\\n\", class_distribution)\n",
    "# class_distribution.plot(kind='bar', title=\"Class Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      " class\n",
      "0    2788\n",
      "1    1813\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset 3\n",
    "d3 = openml.datasets.get_dataset(44)\n",
    "X, y, _, _ = d3.get_data(target=d3.default_target_attribute)\n",
    "\n",
    "# print(X.dtypes)\n",
    "# print('Dane:', X.head())\n",
    "# print('Target: ', y.head())\n",
    "\n",
    "class_distribution = y.value_counts()\n",
    "print(\"Class distribution:\\n\", class_distribution)\n",
    "# class_distribution.plot(kind='bar', title=\"Class Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      " Class1\n",
      "False    1655\n",
      "True      762\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset 4\n",
    "d4 = openml.datasets.get_dataset(40597)\n",
    "X, y, _, _ = d4.get_data(target=\"Class1\")\n",
    "X = X.drop(columns=['Class2', 'Class3', 'Class4', 'Class5', 'Class6', 'Class7', 'Class8', 'Class9'])\n",
    "\n",
    "# print(X.dtypes)\n",
    "# print('Dane:', X.head())\n",
    "# print('Target: ', y.head())\n",
    "\n",
    "class_distribution = y.value_counts()\n",
    "print(\"Class distribution:\\n\", class_distribution)\n",
    "# class_distribution.plot(kind='bar', title=\"Class Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters spaces (or not) and test type\n",
    "\n",
    "'''\n",
    "# This was when I was testing SVM, ignore \n",
    "\n",
    "param_distribution = {\n",
    "    #'model__C': loguniform(2**-10, 2**10),\n",
    "    #'model__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    #'model__degree': [2, 3, 4, 5],\n",
    "    'model__gamma': loguniform(2**-10, 2**10)\n",
    "}\n",
    "\n",
    "param_space = {\n",
    "    #'model__C': Real(2**-10, 2**10, prior='log-uniform', dtype=float),\n",
    "    #'model__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    #'model__degree': [2, 3, 4, 5],\n",
    "    'model__gamma': Real(2**-10, 2**10, prior='log-uniform', dtype=float),\n",
    "}\n",
    "'''\n",
    "param_distribution = {\n",
    "    'model__C': loguniform(2**-10, 2**10),\n",
    "    'model__l1_ratio': uniform(0, 1)\n",
    "}\n",
    "\n",
    "param_space = {\n",
    "    'model__C': Real(2**-10, 2**10, prior='log-uniform', dtype=float),\n",
    "    'model__l1_ratio': Real(0, 1, dtype=float)\n",
    "}\n",
    "\n",
    "test_type = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model__C': 0.20736406818487657, 'model__l1_ratio': 0.4971680515517577}\n"
     ]
    }
   ],
   "source": [
    "# Retrieving defaults (only for hyperparameter tuning, the id comes from the bottom of the file)\n",
    "\n",
    "id = 100\n",
    "\n",
    "df = pd.read_csv(\"_random_iter_res_0.csv\", index_col=0)\n",
    "df['params'] = df['params'].apply(ast.literal_eval)\n",
    "defaults = df.loc[id]['params']\n",
    "\n",
    "print(defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting model (remeber to (not) set appropriate defaults)\n",
    "\n",
    "my_model = LogisticRegression(penalty='elasticnet', solver='saga', class_weight='balanced', n_jobs=-1, max_iter=200, l1_ratio=defaults[\"model__l1_ratio\"])\n",
    "\n",
    "# This was when I was testing SVM, ignore \n",
    "# my_model = SVM = SVC(class_weight='balanced', max_iter=20000)\n",
    "\n",
    "model_pipe = Pipeline([('preprocessing', col_trans), ('model', my_model)])\n",
    "# model_pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_train = 5\n",
    "cv_test = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting optimizers, data and test_type\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    model_pipe,\n",
    "    param_distribution,\n",
    "    n_iter=180,\n",
    "    scoring='roc_auc',\n",
    "    cv=cv_train,\n",
    "    n_jobs=-1, \n",
    "    random_state=rn_st,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "bayes_search = BayesSearchCV(\n",
    "    model_pipe,\n",
    "    param_space,\n",
    "    n_iter=60,\n",
    "    scoring='roc_auc',\n",
    "    cv=cv_train,\n",
    "    n_jobs=-1,\n",
    "    random_state=rn_st,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "data = [(d1, \"class\"), (d2, \"class\"), (d3, \"class\"), (d4, \"Class1\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bakaczor\\AppData\\Local\\Temp\\ipykernel_8856\\1897312603.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  main_results_df = pd.concat([main_results_df, new_row], ignore_index=True)\n",
      "C:\\Users\\Bakaczor\\AppData\\Local\\Temp\\ipykernel_8856\\1897312603.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  main_results_df = pd.concat([main_results_df, new_row], ignore_index=True)\n",
      "d:\\development\\miniconda3\\envs\\automl\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "d:\\development\\miniconda3\\envs\\automl\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "d:\\development\\miniconda3\\envs\\automl\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "d:\\development\\miniconda3\\envs\\automl\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Bakaczor\\AppData\\Local\\Temp\\ipykernel_8856\\1897312603.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  main_results_df = pd.concat([main_results_df, new_row], ignore_index=True)\n",
      "d:\\development\\miniconda3\\envs\\automl\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "d:\\development\\miniconda3\\envs\\automl\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "d:\\development\\miniconda3\\envs\\automl\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "d:\\development\\miniconda3\\envs\\automl\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Bakaczor\\AppData\\Local\\Temp\\ipykernel_8856\\1897312603.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  main_results_df = pd.concat([main_results_df, new_row], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for d, t in data:\n",
    "    main_results_df = pd.DataFrame(columns=['method','elapsed_time', 'best_score', 'test_score', 'best_params', 'auc_score', 'cv_auc_score'])\n",
    "    X, y, _, _ = d.get_data(target=t)\n",
    "    y = target_transformer.fit_transform(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rn_st)\n",
    "    \n",
    "    # Random Search\n",
    "    start_time = time()\n",
    "    random_search.fit(X_train, y_train)\n",
    "    elapsed_time = time() - start_time\n",
    "    \n",
    "    best_score = random_search.best_score_\n",
    "    test_score = random_search.score(X_test, y_test)\n",
    "    best_params = str(random_search.best_params_)\n",
    "    \n",
    "    best_model = random_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    auc_scr = roc_auc_score(y_test, y_pred)\n",
    "    cv_auc_scr = np.mean(cross_val_score(best_model, X_test, y_test, scoring='roc_auc', cv=cv_test))\n",
    "    \n",
    "    random_results = pd.DataFrame(random_search.cv_results_)\n",
    "    \n",
    "    new_row = pd.DataFrame({\n",
    "        'method': \"RandomSearchCV\",\n",
    "        'elapsed_time': [elapsed_time],\n",
    "        'best_score': [best_score],\n",
    "        'test_score': [test_score],\n",
    "        'best_params': [best_params],\n",
    "        'auc_score': [auc_scr],\n",
    "        'cv_auc_score': [cv_auc_scr]\n",
    "    })\n",
    "    main_results_df = pd.concat([main_results_df, new_row], ignore_index=True)\n",
    "    \n",
    "    random_results.to_csv(f\"{test_type}_random_iter_res_{i}.csv\", index=True, index_label='index')\n",
    "    main_results_df.to_csv(f\"{test_type}_main_res_{i}.csv\", index=False)\n",
    "    \n",
    "    # Bayes Search    \n",
    "    start_time = time()\n",
    "    bayes_search.fit(X_train, y_train)\n",
    "    elapsed_time = time() - start_time\n",
    "    \n",
    "    best_score = bayes_search.best_score_\n",
    "    test_score = bayes_search.score(X_test, y_test)\n",
    "    best_params = str(bayes_search.best_params_)\n",
    "    \n",
    "    best_model = bayes_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    auc_scr = roc_auc_score(y_test, y_pred)\n",
    "    cv_auc_scr = np.mean(cross_val_score(best_model, X_test, y_test, scoring='roc_auc', cv=cv_test))\n",
    "    \n",
    "    bayes_results = pd.DataFrame(bayes_search.cv_results_)\n",
    "    \n",
    "    new_row = pd.DataFrame({\n",
    "        'method': \"BayesSearchCV\",\n",
    "        'elapsed_time': [elapsed_time],\n",
    "        'best_score': [best_score],\n",
    "        'test_score': [test_score],\n",
    "        'best_params': [best_params],\n",
    "        'auc_score': [auc_scr],\n",
    "        'cv_auc_score': [cv_auc_scr]\n",
    "    })\n",
    "    main_results_df = pd.concat([main_results_df, new_row], ignore_index=True)\n",
    "    \n",
    "    bayes_results.to_csv(f\"{test_type}_bayes_iter_res_{i}.csv\", index=True, index_label='index')\n",
    "    main_results_df.to_csv(f\"{test_type}_main_res_{i}.csv\", index=False)\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 100, smallest value: -0.8408558276326872\n",
      "\n",
      "Index: 100, smallest value: 0.00683278311908349\n",
      "\n",
      "Index: 140, smallest value: -0.36484708470845584\n",
      "\n",
      "<bound method NDFrame.head of        mean_test_score_0  mean_test_score_1  mean_test_score_2  \\\n",
      "index                                                            \n",
      "0              -0.765192          -0.826719          -0.970711   \n",
      "1              -0.765230          -0.826719          -0.970715   \n",
      "2              -0.779753          -0.825642          -0.966506   \n",
      "3              -0.781558          -0.827946          -0.970641   \n",
      "4              -0.774049          -0.826950          -0.970779   \n",
      "...                  ...                ...                ...   \n",
      "175            -0.500000          -0.500000          -0.610859   \n",
      "176            -0.765305          -0.826719          -0.970683   \n",
      "177            -0.771397          -0.825227          -0.969749   \n",
      "178            -0.766092          -0.826719          -0.970720   \n",
      "179            -0.500000          -0.500000          -0.940542   \n",
      "\n",
      "       mean_test_score_3  mean_test_score_01_0  mean_test_score_01_1  \\\n",
      "index                                                                  \n",
      "0              -0.775808              0.070890              0.006417   \n",
      "1              -0.775808              0.070758              0.006417   \n",
      "2              -0.783087              0.019878              0.009692   \n",
      "3              -0.778338              0.013551              0.002687   \n",
      "4              -0.776499              0.039860              0.005714   \n",
      "...                  ...                   ...                   ...   \n",
      "175            -0.500000              1.000000              1.000000   \n",
      "176            -0.775820              0.070495              0.006417   \n",
      "177            -0.784706              0.049152              0.010956   \n",
      "178            -0.775888              0.067739              0.006417   \n",
      "179            -0.752147              1.000000              1.000000   \n",
      "\n",
      "       mean_test_score_01_2  mean_test_score_01_3  mean_test_score_std_0  \\\n",
      "index                                                                      \n",
      "0                  0.000260              0.037463              -0.385285   \n",
      "1                  0.000250              0.037463              -0.385707   \n",
      "2                  0.009191              0.012060              -0.548508   \n",
      "3                  0.000408              0.028636              -0.568750   \n",
      "4                  0.000115              0.035052              -0.484570   \n",
      "...                     ...                   ...                    ...   \n",
      "175                0.764547              1.000000               2.587580   \n",
      "176                0.000319              0.037420              -0.386546   \n",
      "177                0.002303              0.006412              -0.454839   \n",
      "178                0.000240              0.037183              -0.395365   \n",
      "179                0.064335              0.120039               2.587580   \n",
      "\n",
      "       mean_test_score_std_1  mean_test_score_std_2  mean_test_score_std_3  \\\n",
      "index                                                                        \n",
      "0                  -0.339266              -0.233911              -0.186227   \n",
      "1                  -0.339266              -0.234012              -0.186227   \n",
      "2                  -0.327657              -0.143065              -0.328825   \n",
      "3                  -0.352487              -0.232408              -0.235778   \n",
      "4                  -0.341758              -0.235384              -0.199762   \n",
      "...                      ...                    ...                    ...   \n",
      "175                 3.182363               7.540531               5.216885   \n",
      "176                -0.339266              -0.233309              -0.186469   \n",
      "177                -0.323180              -0.213135              -0.360528   \n",
      "178                -0.339266              -0.234112              -0.187800   \n",
      "179                 3.182363               0.417868               0.277303   \n",
      "\n",
      "       mean_of_mean_test_score  mean_of_mean_test_score_01  \\\n",
      "index                                                        \n",
      "0                    -0.834608                    0.028758   \n",
      "1                    -0.834618                    0.028722   \n",
      "2                    -0.838747                    0.012705   \n",
      "3                    -0.839621                    0.011320   \n",
      "4                    -0.837069                    0.020185   \n",
      "...                        ...                         ...   \n",
      "175                  -0.527715                    0.941137   \n",
      "176                  -0.834632                    0.028663   \n",
      "177                  -0.837770                    0.017206   \n",
      "178                  -0.834855                    0.027895   \n",
      "179                  -0.673172                    0.546093   \n",
      "\n",
      "       mean_of_mean_test_score_std  \n",
      "index                               \n",
      "0                        -0.286172  \n",
      "1                        -0.286303  \n",
      "2                        -0.337014  \n",
      "3                        -0.347356  \n",
      "4                        -0.315369  \n",
      "...                            ...  \n",
      "175                       4.631840  \n",
      "176                      -0.286397  \n",
      "177                      -0.337920  \n",
      "178                      -0.289136  \n",
      "179                       1.616279  \n",
      "\n",
      "[180 rows x 15 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Establish defaults\n",
    "\n",
    "file_list = [f\"_random_iter_res_{i}.csv\" for i in range(4)] \n",
    "\n",
    "dfs = []\n",
    "for file in file_list:\n",
    "    df = pd.read_csv(file, index_col=0)\n",
    "    df['mean_test_score'] = -df['mean_test_score']\n",
    "    \n",
    "    mean = df['mean_test_score'].mean()\n",
    "    std = df['mean_test_score'].std()\n",
    "    min_score = df['mean_test_score'].min()\n",
    "    max_score = df['mean_test_score'].max()\n",
    "    \n",
    "    df['mean_test_score_01'] = (df['mean_test_score'] - min_score) / (max_score - min_score)\n",
    "    df['mean_test_score_std'] = (df['mean_test_score'] - mean) / std\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "mean_test_score = [df['mean_test_score'] for df in dfs]\n",
    "mean_test_score_01 = [df['mean_test_score_01'] for df in dfs]\n",
    "mean_test_score_std = [df['mean_test_score_std'] for df in dfs]\n",
    "\n",
    "scores_df = pd.concat(mean_test_score + mean_test_score_01 + mean_test_score_std, axis=1)\n",
    "\n",
    "scores_df.columns = [f\"mean_test_score_{i}\" for i in range(len(file_list))] + \\\n",
    "                    [f\"mean_test_score_01_{i}\" for i in range(len(file_list))] + \\\n",
    "                    [f\"mean_test_score_std_{i}\" for i in range(len(file_list))]\n",
    "\n",
    "scores_df['mean_of_mean_test_score'] = scores_df[[f\"mean_test_score_{i}\" for i in range(len(file_list))]].mean(axis=1)\n",
    "scores_df['mean_of_mean_test_score_01'] = scores_df[[f\"mean_test_score_01_{i}\" for i in range(len(file_list))]].mean(axis=1)\n",
    "scores_df['mean_of_mean_test_score_std'] = scores_df[[f\"mean_test_score_std_{i}\" for i in range(len(file_list))]].mean(axis=1)\n",
    "\n",
    "min_index = scores_df['mean_of_mean_test_score'].idxmin()\n",
    "min_index_01 = scores_df['mean_of_mean_test_score_01'].idxmin()\n",
    "min_index_std = scores_df['mean_of_mean_test_score_std'].idxmin()\n",
    "\n",
    "print(f\"Index: {min_index}, smallest value: {scores_df.loc[min_index, 'mean_of_mean_test_score']}\\n\")\n",
    "print(f\"Index: {min_index_01}, smallest value: {scores_df.loc[min_index, 'mean_of_mean_test_score_01']}\\n\")\n",
    "print(f\"Index: {min_index_std}, smallest value: {scores_df.loc[min_index, 'mean_of_mean_test_score_std']}\\n\")\n",
    "\n",
    "print(scores_df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving defaults (again, for convenience)\n",
    "id = 100\n",
    "\n",
    "df = pd.read_csv(\"_random_iter_res_0.csv\", index_col=0)\n",
    "df['params'] = df['params'].apply(ast.literal_eval)\n",
    "defaults = df.loc[id]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\development\\miniconda3\\envs\\automl\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "d:\\development\\miniconda3\\envs\\automl\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "d:\\development\\miniconda3\\envs\\automl\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "d:\\development\\miniconda3\\envs\\automl\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "d:\\development\\miniconda3\\envs\\automl\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7246553122465531, 0.7139636530940878), (0.7332202111613877, 0.8291041906958861), (0.9267915828714377, 0.9558960660710003), (0.7076545751831891, 0.7109510049278877)]\n"
     ]
    }
   ],
   "source": [
    "# Scoring with default hyperparameters\n",
    "def_model = LogisticRegression(penalty='elasticnet', solver='saga', class_weight='balanced', n_jobs=-1, max_iter=200, C=defaults[\"model__C\"], l1_ratio=defaults[\"model__l1_ratio\"])\n",
    "def_model_pipe = Pipeline([('preprocessing', col_trans), ('model', def_model)])\n",
    "\n",
    "default_results =[]\n",
    "for d, t in data:\n",
    "    X, y, _, _ = d.get_data(target=t)\n",
    "    y = target_transformer.fit_transform(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rn_st)\n",
    "\n",
    "    def_model_pipe.fit(X_train, y_train)\n",
    "    y_pred = def_model_pipe.predict(X_test)\n",
    "    auc_scr = roc_auc_score(y_test, y_pred)\n",
    "    cv_auc_scr = np.mean(cross_val_score(def_model_pipe, X_test, y_test, scoring='roc_auc', cv=cv_test))\n",
    "    default_results.append((auc_scr, cv_auc_scr))\n",
    "    \n",
    "print(default_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   random_score  bayes_score  time_ratio\n",
      "0      0.006662     0.003448    9.787245\n",
      "1      0.009163     0.011983   19.329395\n",
      "2      0.002074     0.002221    3.971060\n",
      "3     -0.001674    -0.001855    4.836018\n",
      "   random_score  bayes_score  time_ratio\n",
      "0      0.014599     0.014599    7.623124\n",
      "1      0.012047     0.012079   18.126382\n",
      "2      0.000221     0.000354    3.339638\n",
      "3      0.000422     0.000543    7.649208\n",
      "   random_score  bayes_score  time_ratio\n",
      "0      0.009069     0.018248   11.824899\n",
      "1      0.011983     0.010285   21.963220\n",
      "2      0.002045     0.002472    4.868045\n",
      "3      0.001361    -0.001639    5.561116\n",
      "\n",
      "  test_type  random_mean  random_median  bayes_mean  bayes_median  time_mean  \\\n",
      "0     C_140     0.004056       0.004368    0.003949      0.002835   9.480929   \n",
      "1    l1_140     0.006822       0.006235    0.006893      0.006311   9.184588   \n",
      "2               0.006114       0.005557    0.007341      0.006378  11.054320   \n",
      "\n",
      "   time_median  \n",
      "0     7.311632  \n",
      "1     7.636166  \n",
      "2     8.693008  \n"
     ]
    }
   ],
   "source": [
    "# Calculating tunability\n",
    "\n",
    "def parse_best_params(param_str):\n",
    "    # Removes \"OrderedDict(\" and \")\" to make it parsable by ast.literal_eval\n",
    "    cleaned_str = param_str.replace(\"OrderedDict(\", \"\").rstrip(\")\")\n",
    "    return ast.literal_eval(cleaned_str)\n",
    "\n",
    "random_diffs = []\n",
    "bayes_diffs = []\n",
    "time_diffs = []\n",
    "total_diffs_df = pd.DataFrame()\n",
    "\n",
    "test_types = [f\"C_{id}\", f\"l1_{id}\", \"\"]\n",
    "\n",
    "for test_type in test_types:\n",
    "    random_diffs.clear()\n",
    "    bayes_diffs.clear()\n",
    "    time_diffs.clear()\n",
    "    \n",
    "    i=0\n",
    "    for d, t in data:\n",
    "        X, y, _, _ = d.get_data(target=t)\n",
    "        y = target_transformer.fit_transform(y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rn_st)\n",
    "        \n",
    "        file_path = f'{test_type}_main_res_{i}.csv'\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        random_search = df[df['method'] == 'RandomSearchCV'].iloc[0]\n",
    "        bayes_search = df[df['method'] == 'BayesSearchCV'].iloc[0]\n",
    "        \n",
    "        time_diffs.append(3 * bayes_search['elapsed_time'] / random_search['elapsed_time']) # 3 times, because 180 = 3 * 60\n",
    "        \n",
    "        def_auc = -max(default_results[i][0], default_results[i][1])\n",
    "        random_diffs.append(def_auc + max(random_search['auc_score'], random_search['cv_auc_score']))\n",
    "        bayes_diffs.append(def_auc + max(bayes_search['auc_score'], bayes_search['cv_auc_score']))\n",
    "        \n",
    "        '''\n",
    "        # This was just to check if the results will be the same (they are)\n",
    "        # I don't like repeating the code so...\n",
    "        for met in ['RandomSearchCV', 'BayesSearchCV']:\n",
    "            best_params = (df[df['method'] == met]['best_params']).apply(parse_best_params)\n",
    "            idx = 0 if met == 'RandomSearchCV' else 1\n",
    "            C = best_params[idx].get('model__C', None)\n",
    "            l1_ratio = best_params[idx].get('model__l1_ratio', None)\n",
    "            \n",
    "            # Unfortunately there are many combinations\n",
    "            if C is not None and l1_ratio is not None:\n",
    "                def_model = LogisticRegression(penalty='elasticnet', solver='saga', class_weight='balanced', n_jobs=-1, max_iter=200, C=C, l1_ratio=l1_ratio)\n",
    "            elif C is not None:\n",
    "                def_model = LogisticRegression(penalty='elasticnet', solver='saga', class_weight='balanced', n_jobs=-1, max_iter=200, C=C, l1_ratio=defaults[\"model__l1_ratio\"])\n",
    "            else:\n",
    "                def_model = LogisticRegression(penalty='elasticnet', solver='saga', class_weight='balanced', n_jobs=-1, max_iter=200, C=defaults[\"model__C\"], l1_ratio=l1_ratio)\n",
    "            def_model_pipe = Pipeline([('preprocessing', col_trans), ('model', def_model)])\n",
    "            \n",
    "            def_model_pipe.fit(X_train, y_train)\n",
    "            y_pred = def_model_pipe.predict(X_test)\n",
    "            auc_scr = roc_auc_score(y_test, y_pred)\n",
    "        \n",
    "            if met == 'RandomSearchCV':\n",
    "                random_diffs.append(default_results[i] - auc_scr)\n",
    "            else:\n",
    "                bayes_diffs.append(default_results[i] - auc_scr)\n",
    "        # End loop\n",
    "        '''\n",
    "        i += 1\n",
    "\n",
    "    diffs_df = pd.DataFrame({\n",
    "        'random_score': random_diffs,\n",
    "        'bayes_score': bayes_diffs,\n",
    "        'time_ratio': time_diffs\n",
    "    })\n",
    "    \n",
    "    print(diffs_df)\n",
    "    \n",
    "    summary = {\n",
    "        'RandomSearchCV': {\n",
    "            'mean': diffs_df['random_score'].mean(),\n",
    "            'median': diffs_df['random_score'].median()\n",
    "        },\n",
    "        'BayesSearchCV': {\n",
    "            'mean': diffs_df['bayes_score'].mean(),\n",
    "            'median': diffs_df['bayes_score'].median()\n",
    "        },\n",
    "        'Time': {\n",
    "            'mean': diffs_df['time_ratio'].mean(),\n",
    "            'median': diffs_df['time_ratio'].median()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_row = pd.DataFrame({\n",
    "        'test_type': [test_type],\n",
    "        'random_mean': [summary['RandomSearchCV']['mean']],\n",
    "        'random_median': [summary['RandomSearchCV']['median']],\n",
    "        'bayes_mean': [summary['BayesSearchCV']['mean']],\n",
    "        'bayes_median': [summary['BayesSearchCV']['median']],\n",
    "        'time_mean': [summary['Time']['mean']],\n",
    "        'time_median': [summary['Time']['median']]\n",
    "    })\n",
    "    \n",
    "    total_diffs_df = pd.concat([total_diffs_df, summary_row], ignore_index=True)\n",
    "\n",
    "print()\n",
    "print(total_diffs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   relative_random_mean  relative_bayes_mean  relative_random_median  \\\n",
      "0              0.663420             0.537955                0.786059   \n",
      "1              1.115743             0.939003                1.121902   \n",
      "\n",
      "   relative_bayes_median  \n",
      "0               0.444484  \n",
      "1               0.989459  \n"
     ]
    }
   ],
   "source": [
    "base_values = total_diffs_df[total_diffs_df['test_type'] == ''].iloc[0]\n",
    "relative_df = pd.DataFrame()\n",
    "\n",
    "for column in ['random_mean', 'bayes_mean', 'random_median', 'bayes_median']:\n",
    "    relative_df[f'relative_{column}'] = total_diffs_df[column] / base_values[column]\n",
    "\n",
    "relative_df = relative_df.iloc[:-1]\n",
    "\n",
    "print(relative_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
