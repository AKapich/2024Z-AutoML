{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn import set_config\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "\n",
    "\n",
    "from shared import RANDOM_STATE, DIABETES_DATASET_ID, BANKNOTE_DATASET_ID, CREDIT_DATASET_ID, SPAMBASE_DATASET_ID\n",
    "from shared.utilities import prepare_and_split, create_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = RANDOM_STATE\n",
    "output_path =\"..\\\\results\\\\gradient-boost\\\\random_search_results.csv\"\n",
    "bayes_output_path = \"..\\\\results\\\\gradient-boost\\\\bayes_search_results.csv\"\n",
    "set_config(transform_output = \"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ESTIMATORS_MIN = 10\n",
    "N_ESTIMATORS_MAX = 400\n",
    "DEPTH_MIN = 3\n",
    "DEPTH_MAX = 15\n",
    "SPLIT_SAMPLES_MIN = 2\n",
    "SPLIT_SAMPLES_MAX = 10\n",
    "LEAF_SAMPLES_MIN = 1\n",
    "LEAF_SAMPLES_MAX = 10\n",
    "SPLIT_TYPE = ['sqrt', 'log2', 0.2, 0.4]\n",
    "\n",
    "N_JOBS = -1\n",
    "N_ITERS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_dataset = fetch_openml(data_id=DIABETES_DATASET_ID, as_frame=True)\n",
    "banknote_authentication_dataset = fetch_openml(data_id=BANKNOTE_DATASET_ID, as_frame=True)\n",
    "credit_dataset = fetch_openml(data_id=CREDIT_DATASET_ID, as_frame=True)\n",
    "spambase_dataset = fetch_openml(data_id=SPAMBASE_DATASET_ID, as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_train_x, diabetes_train_y= prepare_and_split(diabetes_dataset)\n",
    "banknotes_train_x, banknotes_train_y= prepare_and_split(banknote_authentication_dataset)\n",
    "credit_train_x, credit_train_y= prepare_and_split(credit_dataset)\n",
    "spambase_train_x, spambase_train_y= prepare_and_split(spambase_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'model__n_estimators': np.arange(N_ESTIMATORS_MIN, N_ESTIMATORS_MAX + 1, 10).astype(int),\n",
    "    'model__max_depth': np.linspace(DEPTH_MIN, DEPTH_MAX, DEPTH_MAX- DEPTH_MIN).astype(int),\n",
    "    'model__min_samples_split': np.linspace(SPLIT_SAMPLES_MIN, SPLIT_SAMPLES_MAX, SPLIT_SAMPLES_MAX - SPLIT_SAMPLES_MIN).astype(int),\n",
    "    'model__min_samples_leaf': np.linspace(LEAF_SAMPLES_MIN, LEAF_SAMPLES_MAX, LEAF_SAMPLES_MAX - LEAF_SAMPLES_MIN).astype(int),\n",
    "    'model__max_features': SPLIT_TYPE \n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessing', create_preprocessor()),\n",
    "    ('model', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "randomized_search_CV = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_jobs=N_JOBS,\n",
    "    n_iter=N_ITERS,\n",
    "    verbose=1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    error_score='raise'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    (diabetes_train_x, diabetes_train_y, \"diabetes\"),\n",
    "    (banknotes_train_x, banknotes_train_y, \"banknotes\"),\n",
    "    (credit_train_x, credit_train_y, \"credit\"),\n",
    "    (spambase_train_x, spambase_train_y, \"spambase\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_results = []\n",
    "\n",
    "for x, y, name in datasets:\n",
    "    randomized_search_CV.fit(x, y)\n",
    "    cv_results = randomized_search_CV.cv_results_\n",
    "    df_results = pd.DataFrame(cv_results)\n",
    "    df_results['dataset'] = name\n",
    "    random_search_results.append(df_results)\n",
    "    \n",
    "all_results = pd.concat(random_search_results)\n",
    "all_results.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_combination_scores(results_df):\n",
    "\n",
    "    grouped = results_df.groupby(['param_model__n_estimators', \n",
    "                                'param_model__min_samples_split',\n",
    "                                'param_model__min_samples_leaf',\n",
    "                                'param_model__max_features',\n",
    "                                'param_model__max_depth'])\n",
    "    \n",
    "    average_scores = grouped.agg({\n",
    "        'mean_test_score': ['mean', 'std'],\n",
    "    }).round(4)\n",
    "    \n",
    "    average_scores.columns = ['average_score', 'std_between_datasets']\n",
    "    \n",
    "    average_scores = average_scores.reset_index()\n",
    "    \n",
    "    return average_scores.sort_values('average_score', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_hyperparams(results_df):\n",
    "    best_params = results_df.sort_values('average_score', ascending=False).iloc[0]\n",
    "    \n",
    "    return {\n",
    "        'n_estimators': best_params['param_model__n_estimators'],\n",
    "        'min_samples_split': best_params['param_model__min_samples_split'],\n",
    "        'min_samples_leaf': best_params['param_model__min_samples_leaf'],\n",
    "        'max_features': best_params['param_model__max_features'],\n",
    "        'max_depth': best_params['param_model__max_depth'],\n",
    "        'average_score': best_params['average_score']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_with_avg_scores = get_average_combination_scores(all_results)\n",
    "print(\"Best hyperparameters:\")\n",
    "results_with_avg_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Worst hyperparameters:\")\n",
    "results_with_avg_scores.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.groupby('dataset').agg({'mean_test_score': 'max'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default params score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "scoring = make_scorer(accuracy_score)\n",
    "for x, y, name in datasets:\n",
    "    result = cross_validate(pipeline, x, y, cv=5, scoring=scoring)['test_score'].mean()\n",
    "    \n",
    "    results.append({'dataset': name, 'score': result})\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smac.scenario import Scenario\n",
    "from ConfigSpace import ConfigurationSpace\n",
    "from ConfigSpace.hyperparameters import UniformIntegerHyperparameter, CategoricalHyperparameter\n",
    "from smac.facade.hyperparameter_optimization_facade import HyperparameterOptimizationFacade\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_results_log = []\n",
    "def objective_function(config, seed = RANDOM_STATE):\n",
    "    scores = []\n",
    "    \n",
    "    for x,y, name in datasets:\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessing', create_preprocessor()),\n",
    "            ('model', GradientBoostingClassifier())\n",
    "        ])\n",
    "        \n",
    "        pipeline.set_params(\n",
    "            model__n_estimators=config['n_estimators'],\n",
    "            model__max_depth=config['max_depth'],\n",
    "            model__min_samples_split=config['min_samples_split'],\n",
    "            model__min_samples_leaf=config['min_samples_leaf'],\n",
    "            model__max_features=config['max_features'],\n",
    "        )\n",
    "        \n",
    "        score = cross_val_score(pipeline, x, y, cv=5, scoring='roc_auc').mean()\n",
    "        scores.append({'score': score, 'dataset': name})\n",
    "            \n",
    "        bayes_results_log.append({\n",
    "            'n_estimators': config['n_estimators'],\n",
    "            'max_depth': config['max_depth'],\n",
    "            'min_samples_split': config['min_samples_split'],\n",
    "            'min_samples_leaf': config['min_samples_leaf'],\n",
    "            'max_features': config['max_features'],\n",
    "            'score': score,\n",
    "            'dataset': name\n",
    "        })\n",
    "\n",
    "    scores_mean = np.mean([s['score'] for s in scores])    \n",
    "    return 1-scores_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = ConfigurationSpace()\n",
    "\n",
    "n_estimators = CategoricalHyperparameter(\"n_estimators\", np.arange(N_ESTIMATORS_MIN, N_ESTIMATORS_MAX + 1, 10).tolist())\n",
    "max_depth = UniformIntegerHyperparameter(\"max_depth\", DEPTH_MIN, DEPTH_MAX, default_value=10)\n",
    "min_samples_split = UniformIntegerHyperparameter(\"min_samples_split\", SPLIT_SAMPLES_MIN, SPLIT_SAMPLES_MAX, default_value=2)\n",
    "min_samples_leaf = UniformIntegerHyperparameter(\"min_samples_leaf\", LEAF_SAMPLES_MIN, LEAF_SAMPLES_MAX, default_value=1)\n",
    "max_features = CategoricalHyperparameter(\"max_features\", SPLIT_TYPE, default_value=\"sqrt\")\n",
    "\n",
    "cs.add([n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features])\n",
    "\n",
    "default_cfg = cs.get_default_configuration()\n",
    "scenario = Scenario(cs, deterministic=True, n_trials=N_ITERS)\n",
    "\n",
    "smac = HyperparameterOptimizationFacade(scenario, objective_function)\n",
    "bayes_best_hiperparameters = smac.optimize()\n",
    "\n",
    "pd.DataFrame(bayes_results_log).to_csv(bayes_output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(bayes_results_log).groupby('dataset').agg({'score': 'max'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
